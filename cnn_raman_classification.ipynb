{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Classification of Raman Spectra (ramanbiolib)\n",
    "\n",
    "This notebook:\n",
    "1. Loads and prepares the preprocessed Raman spectra dataset from `ramanbiolib`\n",
    "2. Trains a **1D CNN** for multi-class molecular classification\n",
    "3. Evaluates performance (accuracy, confusion matrix, per-class F1)\n",
    "4. Applies **Integrated Gradients** to identify which wavenumber regions drive predictions\n",
    "5. Plots saliency maps overlaid on example spectra for each class\n",
    "6. Saves model weights and training logs to `outputs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 1: Imports ─────────────────────────────────────────────────────────\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # headless backend so plots save without a display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "# Output directories\n",
    "os.makedirs('outputs/figures', exist_ok=True)\n",
    "os.makedirs('outputs/logs',    exist_ok=True)\n",
    "os.makedirs('outputs/model',   exist_ok=True)\n",
    "print('Output directories ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 2: Load spectra + metadata ─────────────────────────────────────────\n",
    "def parse_list(s):\n",
    "    \"\"\"Convert a '[a, b, c]' string from the CSV into a Python list of floats.\"\"\"\n",
    "    return [float(v) for v in s.strip('[]').split(', ')]\n",
    "\n",
    "spectra_df = pd.read_csv(\n",
    "    'ramanbiolib/db/raman_spectra_db.csv',\n",
    "    converters={'wavenumbers': parse_list, 'intensity': parse_list}\n",
    ")\n",
    "meta_df = pd.read_csv('ramanbiolib/db/metadata_db.csv')\n",
    "\n",
    "# Merge on id — keep only the first duplicate per id (some ids appear multiple\n",
    "# times in metadata with different laser wavelengths)\n",
    "meta_unique = meta_df[['id', 'type']].drop_duplicates(subset='id')\n",
    "df = spectra_df.merge(meta_unique, on='id')\n",
    "\n",
    "# Collapse to top-level class (e.g. 'Lipids/FattyAcids' -> 'Lipids')\n",
    "df['class'] = df['type'].str.split('/').str[0]\n",
    "\n",
    "print('Full dataset shape:', df.shape)\n",
    "print('\\nClass distribution:')\n",
    "print(df['class'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 3: Visualise class distribution ────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "counts = df['class'].value_counts()\n",
    "ax.bar(counts.index, counts.values,\n",
    "       color=plt.cm.tab10(np.linspace(0, 1, len(counts))))\n",
    "ax.set_xlabel('Molecular Class', fontsize=12)\n",
    "ax.set_ylabel('Number of Spectra', fontsize=12)\n",
    "ax.set_title('Class Distribution in ramanbiolib Spectra DB', fontsize=14)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/class_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: outputs/figures/class_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 4: Filter to top-6 classes (enough samples for meaningful training) ─\n",
    "KEEP_CLASSES = ['Proteins', 'Lipids', 'Saccharides',\n",
    "                'AminoAcids', 'PrimaryMetabolites', 'NucleicAcids']\n",
    "\n",
    "df_filt = df[df['class'].isin(KEEP_CLASSES)].reset_index(drop=True)\n",
    "print('Filtered dataset shape:', df_filt.shape)\n",
    "print(df_filt['class'].value_counts().to_string())\n",
    "\n",
    "# Extract numpy arrays\n",
    "X_raw = np.array(df_filt['intensity'].tolist(), dtype=np.float32)   # (N, 1351)\n",
    "wavenumbers = np.array(df_filt['wavenumbers'].iloc[0])               # (1351,)\n",
    "print(f'\\nSpectrum length: {X_raw.shape[1]} wavenumber points')\n",
    "print(f'Wavenumber range: {wavenumbers[0]:.0f} – {wavenumbers[-1]:.0f} cm⁻¹')\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_raw = le.fit_transform(df_filt['class'])\n",
    "CLASS_NAMES = list(le.classes_)\n",
    "N_CLASSES   = len(CLASS_NAMES)\n",
    "print(f'\\nClasses ({N_CLASSES}): {CLASS_NAMES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 5: Augmentation ─────────────────────────────────────────────────────\n",
    "# With only ~194 spectra we augment by adding Gaussian noise and random scaling.\n",
    "# Each original spectrum produces AUG_FACTOR augmented copies.\n",
    "\n",
    "AUG_FACTOR = 15   # multiplier per spectrum\n",
    "\n",
    "def augment_spectra(X, y, factor=AUG_FACTOR, noise_std=0.015, scale_range=(0.85, 1.15)):\n",
    "    \"\"\"Add Gaussian noise + random amplitude scaling to each spectrum.\"\"\"\n",
    "    X_aug, y_aug = [X.copy()], [y.copy()]\n",
    "    rng = np.random.default_rng(42)\n",
    "    for _ in range(factor):\n",
    "        noise  = rng.normal(0, noise_std, X.shape).astype(np.float32)\n",
    "        scales = rng.uniform(*scale_range, (X.shape[0], 1)).astype(np.float32)\n",
    "        X_aug.append(np.clip(X * scales + noise, 0, None))\n",
    "        y_aug.append(y.copy())\n",
    "    return np.vstack(X_aug), np.concatenate(y_aug)\n",
    "\n",
    "X_aug, y_aug = augment_spectra(X_raw, y_raw)\n",
    "print(f'After augmentation: {X_aug.shape[0]} spectra ({N_CLASSES} classes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset, DataLoader, and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 6: Train / test split and PyTorch Dataset ───────────────────────────\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_aug, y_aug, test_size=0.20, stratify=y_aug, random_state=42\n",
    ")\n",
    "print(f'Train: {len(X_train)}  Test: {len(X_test)}')\n",
    "\n",
    "class RamanDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # CNN expects (batch, channels, length) → add channel dim\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = RamanDataset(X_train, y_train)\n",
    "test_ds  = RamanDataset(X_test,  y_test)\n",
    "\n",
    "# Weighted sampler for class balance during training\n",
    "class_counts = np.bincount(y_train)\n",
    "weights = 1.0 / class_counts[y_train]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "\n",
    "BATCH = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False)\n",
    "print(f'Batch size: {BATCH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 1D CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 7: Model definition ─────────────────────────────────────────────────\n",
    "class RamanCNN1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Three-block 1D CNN for Raman spectrum classification.\n",
    "    Input shape: (batch, 1, 1351)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len=1351, n_classes=6):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(1,  32, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.MaxPool1d(4), nn.Dropout(0.25)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.MaxPool1d(4), nn.Dropout(0.25)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.MaxPool1d(4), nn.Dropout(0.25)\n",
    "        )\n",
    "        # Compute flattened size\n",
    "        dummy = torch.zeros(1, 1, input_len)\n",
    "        flat  = self._forward_features(dummy).shape[1]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flat, 256), nn.ReLU(), nn.Dropout(0.4),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "SEQ_LEN = X_train.shape[1]\n",
    "model = RamanCNN1D(input_len=SEQ_LEN, n_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Model parameters: {n_params:,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 8: Training loop ────────────────────────────────────────────────────\n",
    "EPOCHS   = 80\n",
    "LR       = 1e-3\n",
    "WD       = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # ── Train\n",
    "    model.train()\n",
    "    t_loss, t_correct, t_total = 0, 0, 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t_loss    += loss.item() * len(yb)\n",
    "        t_correct += (logits.argmax(1) == yb).sum().item()\n",
    "        t_total   += len(yb)\n",
    "    scheduler.step()\n",
    "\n",
    "    # ── Validate\n",
    "    model.eval()\n",
    "    v_loss, v_correct, v_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "            v_loss    += loss.item() * len(yb)\n",
    "            v_correct += (logits.argmax(1) == yb).sum().item()\n",
    "            v_total   += len(yb)\n",
    "\n",
    "    train_acc = t_correct / t_total\n",
    "    val_acc   = v_correct / v_total\n",
    "    history['train_loss'].append(t_loss / t_total)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(v_loss / v_total)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'outputs/model/best_model.pt')\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f'Epoch {epoch:3d}/{EPOCHS} | '\n",
    "              f'Train loss {t_loss/t_total:.4f} acc {train_acc:.3f} | '\n",
    "              f'Val loss {v_loss/v_total:.4f} acc {val_acc:.3f}')\n",
    "\n",
    "print(f'\\nBest validation accuracy: {best_val_acc:.4f}')\n",
    "\n",
    "# Save training log\n",
    "pd.DataFrame(history).to_csv('outputs/logs/training_log.csv', index=False)\n",
    "print('Training log saved to outputs/logs/training_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 9: Training curves ──────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train', lw=2)\n",
    "axes[0].plot(history['val_loss'],   label='Val',   lw=2)\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Cross-Entropy Loss'); axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train', lw=2)\n",
    "axes[1].plot(history['val_acc'],   label='Val',   lw=2)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Classification Accuracy'); axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: outputs/figures/training_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 10: Load best model & evaluate ──────────────────────────────────────\n",
    "model.load_state_dict(torch.load('outputs/model/best_model.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        logits = model(xb.to(DEVICE))\n",
    "        all_preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(yb.numpy())\n",
    "\n",
    "all_preds  = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test accuracy: {test_acc:.4f}  ({test_acc*100:.1f}%)')\n",
    "print()\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 11: Confusion matrix ─────────────────────────────────────────────────\n",
    "cm_vals = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_vals, display_labels=CLASS_NAMES)\n",
    "disp.plot(ax=ax, colorbar=False, cmap='Blues')\n",
    "ax.set_title('Confusion Matrix – Test Set', fontsize=13)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: outputs/figures/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrated Gradients (Feature Attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 12: Integrated Gradients implementation ──────────────────────────────\n",
    "#\n",
    "# IG(x) = (x - baseline) * ∫₀¹ ∂F(baseline + α*(x-baseline)) / ∂x  dα\n",
    "# Approximated with N_STEPS Riemann sum.\n",
    "\n",
    "def integrated_gradients(model, input_tensor, target_class,\n",
    "                          baseline=None, n_steps=50):\n",
    "    \"\"\"\n",
    "    Compute Integrated Gradients for a single spectrum.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model        : trained RamanCNN1D\n",
    "    input_tensor : torch.Tensor, shape (1, 1, L)\n",
    "    target_class : int\n",
    "    baseline     : torch.Tensor same shape as input_tensor (default: zeros)\n",
    "    n_steps      : int, number of Riemann steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ig : np.ndarray, shape (L,) — attribution per wavenumber\n",
    "    \"\"\"\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "\n",
    "    # Interpolate along straight-line path from baseline to input\n",
    "    alphas       = torch.linspace(0, 1, n_steps).to(DEVICE)\n",
    "    interpolated = torch.stack([\n",
    "        baseline + alpha * (input_tensor - baseline)\n",
    "        for alpha in alphas\n",
    "    ]).squeeze(1)  # (n_steps, 1, L)\n",
    "\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    # Forward + backward\n",
    "    logits = model(interpolated)       # (n_steps, n_classes)\n",
    "    score  = logits[:, target_class].sum()\n",
    "    score.backward()\n",
    "\n",
    "    grads = interpolated.grad.detach()  # (n_steps, 1, L)\n",
    "    avg_grads = grads.mean(dim=0)       # (1, L)\n",
    "\n",
    "    ig = ((input_tensor - baseline).squeeze() * avg_grads.squeeze()).cpu().numpy()\n",
    "    return ig\n",
    "\n",
    "\n",
    "def compute_class_saliency(model, X_class, label, n_samples=10, n_steps=50):\n",
    "    \"\"\"\n",
    "    Average Integrated Gradients over up to n_samples spectra from one class.\n",
    "    Returns the mean absolute attribution per wavenumber.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    igs = []\n",
    "    # Sample a subset for speed\n",
    "    idx = np.random.choice(len(X_class), min(n_samples, len(X_class)), replace=False)\n",
    "    for i in idx:\n",
    "        x = torch.tensor(X_class[i], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        ig = integrated_gradients(model, x, label, n_steps=n_steps)\n",
    "        igs.append(np.abs(ig))\n",
    "    return np.mean(igs, axis=0)\n",
    "\n",
    "print('Integrated Gradients function defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 13: Compute per-class saliency maps ──────────────────────────────────\n",
    "# Use the *original* (non-augmented) spectra for cleaner attributions\n",
    "saliency_maps = {}   # class_name -> mean |IG| array\n",
    "class_spectra  = {}  # class_name -> mean spectrum array\n",
    "\n",
    "N_SALIENCY_SAMPLES = 15   # per-class spectra used for averaging\n",
    "IG_STEPS           = 50\n",
    "\n",
    "for cls_name in CLASS_NAMES:\n",
    "    cls_idx  = le.transform([cls_name])[0]\n",
    "    mask     = y_raw == cls_idx\n",
    "    X_cls    = X_raw[mask]\n",
    "\n",
    "    print(f'Computing IG for {cls_name} ({len(X_cls)} spectra)...', end=' ')\n",
    "    ig_mean = compute_class_saliency(\n",
    "        model, X_cls, cls_idx,\n",
    "        n_samples=N_SALIENCY_SAMPLES, n_steps=IG_STEPS\n",
    "    )\n",
    "    saliency_maps[cls_name] = ig_mean\n",
    "    class_spectra[cls_name] = X_cls.mean(axis=0)\n",
    "    print('done')\n",
    "\n",
    "print('\\nAll saliency maps computed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saliency Maps Overlaid on Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 14: Saliency overlay plots (one plot per class) ─────────────────────\n",
    "COLORS = plt.cm.tab10(np.linspace(0, 1, N_CLASSES))\n",
    "\n",
    "for i, cls_name in enumerate(CLASS_NAMES):\n",
    "    sal   = saliency_maps[cls_name]\n",
    "    spec  = class_spectra[cls_name]\n",
    "    sal_n = (sal - sal.min()) / (sal.max() - sal.min() + 1e-9)  # normalize 0-1\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    # Mean spectrum (left y-axis)\n",
    "    color_spec = COLORS[i]\n",
    "    ax1.plot(wavenumbers, spec, color=color_spec, lw=1.5, label='Mean spectrum')\n",
    "    ax1.set_xlabel('Wavenumber (cm⁻¹)', fontsize=12)\n",
    "    ax1.set_ylabel('Intensity (normalised)', color=color_spec, fontsize=12)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_spec)\n",
    "\n",
    "    # Saliency (right y-axis, shaded)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.fill_between(wavenumbers, sal_n, alpha=0.35, color='crimson', label='IG saliency')\n",
    "    ax2.set_ylabel('Normalised |IG| saliency', color='crimson', fontsize=12)\n",
    "    ax2.tick_params(axis='y', labelcolor='crimson')\n",
    "\n",
    "    # Annotate top-3 wavenumber peaks in saliency\n",
    "    from scipy.signal import find_peaks\n",
    "    peaks, _ = find_peaks(sal_n, prominence=0.15, distance=20)\n",
    "    top_peaks = peaks[np.argsort(sal_n[peaks])[::-1][:3]]\n",
    "    for pk in top_peaks:\n",
    "        ax1.axvline(wavenumbers[pk], color='grey', lw=0.8, ls='--')\n",
    "        ax1.text(wavenumbers[pk] + 5, spec.max() * 0.9,\n",
    "                 f'{wavenumbers[pk]:.0f}', fontsize=8, rotation=90)\n",
    "\n",
    "    # Combined legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=9)\n",
    "\n",
    "    ax1.set_title(f'Saliency Map – {cls_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    out_path = f'outputs/figures/saliency_{cls_name.lower()}.png'\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f'Saved: {out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 15: Aggregate saliency heatmap (all classes) ─────────────────────────\n",
    "sal_matrix = np.array([\n",
    "    (saliency_maps[c] - saliency_maps[c].min()) /\n",
    "    (saliency_maps[c].max() - saliency_maps[c].min() + 1e-9)\n",
    "    for c in CLASS_NAMES\n",
    "])   # (n_classes, 1351)\n",
    "\n",
    "# Downsample wavenumber axis for readability\n",
    "step = 10\n",
    "wn_ds  = wavenumbers[::step]\n",
    "sal_ds = sal_matrix[:, ::step]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "im = ax.imshow(sal_ds, aspect='auto', cmap='hot',\n",
    "               extent=[wn_ds[0], wn_ds[-1], len(CLASS_NAMES)-0.5, -0.5])\n",
    "ax.set_yticks(range(len(CLASS_NAMES)))\n",
    "ax.set_yticklabels(CLASS_NAMES, fontsize=11)\n",
    "ax.set_xlabel('Wavenumber (cm⁻¹)', fontsize=12)\n",
    "ax.set_title('Integrated-Gradient Saliency Heatmap (all classes)', fontsize=13)\n",
    "plt.colorbar(im, ax=ax, label='Normalised |IG|')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/saliency_heatmap_all.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved: outputs/figures/saliency_heatmap_all.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Spectral Regions Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 16: Identify top wavenumber windows per class ───────────────────────\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "WINDOW = 20   # ± cm⁻¹ around each peak to define a 'region'\n",
    "\n",
    "key_regions = {}\n",
    "summary_rows = []\n",
    "\n",
    "for cls_name in CLASS_NAMES:\n",
    "    sal   = saliency_maps[cls_name]\n",
    "    sal_n = (sal - sal.min()) / (sal.max() - sal.min() + 1e-9)\n",
    "\n",
    "    peaks, props = find_peaks(sal_n, prominence=0.10, distance=15)\n",
    "    if len(peaks) == 0:\n",
    "        # Fall back: take absolute max\n",
    "        peaks = np.array([np.argmax(sal_n)])\n",
    "\n",
    "    # Sort by saliency height\n",
    "    ranked = peaks[np.argsort(sal_n[peaks])[::-1]]\n",
    "    top5   = ranked[:5]\n",
    "\n",
    "    regions = []\n",
    "    for pk in top5:\n",
    "        wn_center = wavenumbers[pk]\n",
    "        regions.append({\n",
    "            'center_cm': int(wn_center),\n",
    "            'range': f'{int(wn_center-WINDOW)}–{int(wn_center+WINDOW)} cm⁻¹',\n",
    "            'saliency': float(sal_n[pk])\n",
    "        })\n",
    "        summary_rows.append({\n",
    "            'class': cls_name,\n",
    "            'center_cm': int(wn_center),\n",
    "            'range': f'{int(wn_center-WINDOW)}–{int(wn_center+WINDOW)} cm⁻¹',\n",
    "            'saliency_score': round(float(sal_n[pk]), 4)\n",
    "        })\n",
    "\n",
    "    key_regions[cls_name] = regions\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.to_csv('outputs/logs/key_spectral_regions.csv', index=False)\n",
    "print('Key spectral regions saved to outputs/logs/key_spectral_regions.csv')\n",
    "print()\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Final Model & Artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell 17: Save model weights, config, and saliency arrays ─────────────────\n",
    "\n",
    "# 1. Final weights (at end of training)\n",
    "torch.save(model.state_dict(), 'outputs/model/final_model.pt')\n",
    "\n",
    "# 2. Model config\n",
    "config = {\n",
    "    'input_len': SEQ_LEN,\n",
    "    'n_classes': N_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'wavenumber_range': [int(wavenumbers[0]), int(wavenumbers[-1])],\n",
    "    'epochs': EPOCHS,\n",
    "    'best_val_acc': round(best_val_acc, 4),\n",
    "    'test_acc': round(float(test_acc), 4)\n",
    "}\n",
    "with open('outputs/model/model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# 3. Saliency arrays\n",
    "np.savez('outputs/logs/saliency_maps.npz',\n",
    "         wavenumbers=wavenumbers,\n",
    "         class_names=np.array(CLASS_NAMES),\n",
    "         **{cls: saliency_maps[cls] for cls in CLASS_NAMES})\n",
    "\n",
    "print('Saved artefacts:')\n",
    "print('  outputs/model/final_model.pt')\n",
    "print('  outputs/model/best_model.pt')\n",
    "print('  outputs/model/model_config.json')\n",
    "print('  outputs/logs/training_log.csv')\n",
    "print('  outputs/logs/key_spectral_regions.csv')\n",
    "print('  outputs/logs/saliency_maps.npz')\n",
    "print('  outputs/figures/  (all plots)')\n",
    "\n",
    "print(f'\\n===  FINAL TEST ACCURACY: {test_acc*100:.2f}%  ===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary of Key Spectral Regions\n",
    "\n",
    "| Class | Top Region 1 | Top Region 2 | Top Region 3 |\n",
    "|---|---|---|---|\n",
    "| **Proteins** | ~1650 cm⁻¹ (Amide I) | ~1250 cm⁻¹ (Amide III) | ~1003 cm⁻¹ (Phe ring) |\n",
    "| **Lipids** | ~1300 cm⁻¹ (CH₂ twist) | ~1440 cm⁻¹ (CH₂ scissor) | ~1660 cm⁻¹ (C=C stretch) |\n",
    "| **Saccharides** | ~1340 cm⁻¹ (C-H bending) | ~1460 cm⁻¹ (CH₂) | ~930 cm⁻¹ (C-O-C ring) |\n",
    "| **AminoAcids** | ~1670 cm⁻¹ (C=O) | ~1200 cm⁻¹ (C-N) | ~850 cm⁻¹ (C-C) |\n",
    "| **PrimaryMetabolites** | ~1380 cm⁻¹ | ~1620 cm⁻¹ | ~750 cm⁻¹ |\n",
    "| **NucleicAcids** | ~790 cm⁻¹ (ring breathing) | ~1090 cm⁻¹ (PO₂⁻) | ~1580 cm⁻¹ (base C=N) |\n",
    "\n",
    "> **Note:** The exact wavenumber peaks above are illustrative. Run this notebook to obtain model-derived values stored in `outputs/logs/key_spectral_regions.csv`.\n",
    "\n",
    "See `spectral_regions_summary.md` for a detailed narrative summary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
